
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>SLAM++ · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-multipart/multipart.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-image-captions/image-captions.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-disqus/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-donate/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-terminal/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-codeblock-filename/block.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    


    

        
    
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html" />
    
    
    <link rel="prev" href="Fusion++.html" />
    

    
    <link rel="stylesheet" href="gitbook/gitbook-plugin-chart/c3/c3.min.css">
    <script src="gitbook/gitbook-plugin-chart/c3/d3.min.js"></script>
    <script src="gitbook/gitbook-plugin-chart/c3/c3.min.js"></script>
    

    <script src="gitbook/gitbook-plugin-graph/d3.min.js"></script>
    <script src="gitbook/gitbook-plugin-graph/function-plot.js"></script>    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Overview</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    About
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="SLAM-Introduction.html">
            
                <a href="SLAM-Introduction.html">
            
                    
                    SLAM Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="SLAM-Resources.html">
            
                <a href="SLAM-Resources.html">
            
                    
                    SLAM Resources
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="Software.html">
            
                <a href="Software.html">
            
                    
                    SLAM Software
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Math Foundamentals</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="Coordinate_System.html">
            
                <a href="Coordinate_System.html">
            
                    
                    Coordinate_System
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="Recursive_Bayes_Filter.html">
            
                <a href="Recursive_Bayes_Filter.html">
            
                    
                    Recursive Bayes Filter
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">AI SLAM</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="AI-SLAM-Summary.html">
            
                <a href="AI-SLAM-Summary.html">
            
                    
                    AI SLAM Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="slam-with-objects-using-a-nonparametric-pose-graph.html">
            
                <a href="slam-with-objects-using-a-nonparametric-pose-graph.html">
            
                    
                    Slam with objects using a nonparametric pose graph
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="Fusion++.html">
            
                <a href="Fusion++.html">
            
                    
                    Fusion++
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="3.4" data-path="SLAM++.html">
            
                <a href="SLAM++.html">
            
                    
                    SLAM++
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html">
            
                <a href="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html">
            
                    
                    Monocular Object and Plane SLAM in Structured Environments
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="CubeSLAM.html">
            
                <a href="CubeSLAM.html">
            
                    
                    CubeSLAM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.7" data-path="SSFM.html">
            
                <a href="SSFM.html">
            
                    
                    SSFM
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Traditional SLAM</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="ORB-SLAM.html">
            
                <a href="ORB-SLAM.html">
            
                    
                    ORB-SLAM
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">lidar</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="lidar.html">
            
                <a href="lidar.html">
            
                    
                    lidar
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="graph-slam.html">
            
                <a href="graph-slam.html">
            
                    
                    graph slam
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">ML,DL</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="Mask-RCNN.html">
            
                <a href="Mask-RCNN.html">
            
                    
                    Mask-RCNN
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Tools</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="ros.html">
            
                <a href="ros.html">
            
                    
                    ros
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="kinect2.html">
            
                <a href="kinect2.html">
            
                    
                    kinect2
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Video Courses - Robot & SLAM & AI</li>
        
        
    
        <li class="chapter " data-level="8.1" data-path="小象学院slam无人驾驶学习笔记.html">
            
                <a href="小象学院slam无人驾驶学习笔记.html">
            
                    
                    小象学院slam无人驾驶学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.2" data-path="机器人视觉与控制学习笔记.html">
            
                <a href="机器人视觉与控制学习笔记.html">
            
                    
                    机器人视觉与控制学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.3" data-path="slam-course-robot-mapping.html">
            
                <a href="slam-course-robot-mapping.html">
            
                    
                    SLAM-Course-Robot Mapping
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >SLAM++</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="2015-slam-simultaneous-localisation-and-mapping-at-the-level-of-objecs">2015-SLAM++: Simultaneous Localisation and Mapping at the Level of Objecs</h1>
<h2 id="slam-simultaneous-localisation-and-mapping-at-the-level-of-objecs">SLAM++: Simultaneous Localisation and Mapping at the Level of Objecs</h2>
<p><strong>Authors</strong>: Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H.J. Kelly, <strong>Andrew J. Davison</strong>[1]</p>
<p>[1] Professor, Department of Computing, Imperial College London, vision and AI technology for next generation home robotics</p>
<p><strong>Cite</strong>:The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1352-1359</p>
<hr>
<h2 id="author-introduction-andrew-j-davison">Author Introduction: <strong>Andrew J. Davison</strong></h2>
<p><img align="right" width="30%" height="30%" src="Assets/images/papers/SLAM++/AndrewDavison_photo.png"></p>
<ul>
<li><p>Currently his main research interests are in improving the performance in terms of <strong>dynamics, scale, detail level, efficiency and semantic understanding of real-time 3D vision</strong></p>
</li>
<li><p>He believes that SLAM is evolving into something even more important that I am calling &quot;<strong>Spatial AI</strong>&quot;(Refer:<a href="https://arxiv.org/pdf/1803.11288.pdf" target="_blank"><strong>FutureMapping: The Computational Structure of Spatial AI Systems</strong></a> )</p>
</li>
</ul>
<hr>
<h2 id="overview-demo-youtube">Overview <a href="https://www.youtube.com/watch?v=tmrAh1CqCRo&amp;t=1s" target="_blank">Demo-Youtube</a></h2>
<p><img src="Assets/images/papers/SLAM++/SLAMPlusPlus_YoutubeDemo.png" alt=""></p>
<h2 id="slam---a-pure-object-level-slam-system">SLAM++ - A Pure Object-Level SLAM System</h2>
<p><img align="right" width="60%" height="100%" src="Assets/images/papers/SLAM++/SLAMPlusPlus-chairs.png"></p>
<p> In SLAM++, a cluttered 3D scene is efficiently <strong>tracked and mapped in real-time</strong> directly at the <strong>object level</strong>. &lt;/br&gt;</p>
<p>A new <strong>object oriented</strong> 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures.</p>
<h2 id="abstract">Abstract</h2>
<p>Demonstrate real-time incremental SLAM:</p>
<ul>
<li>in large, cluttered environments, <strong>scenes consit of repeated, domain-specific objects and structures</strong></li>
<li>with relocalisation and loop closure</li>
<li>with the detection of moved objects</li>
<li>with <strong>the generation of an object level scene description</strong></li>
<li>with the potential to enable <strong>interaction</strong></li>
</ul>
<hr>
<h2 id="introduction---current-slam">Introduction - Current SLAM</h2>
<p>Most current real-time SLAM systems operate at the level of</p>
<ul>
<li><strong>low-level primitives</strong> (points, lines, patches or non-parametric surface representations such as depth maps), which must be</li>
<li><strong>robustly matched</strong>,</li>
<li><strong>geometrically transformed</strong></li>
<li>and <strong>optimised over</strong> in order that</li>
<li><strong>maps</strong> represent <strong>the intricacies of the real world</strong>.</li>
</ul>
<hr>
<h2 id="introduction---object-slam">Introduction - Object SLAM</h2>
<ul>
<li>Much interest is now turning to <strong>semantic labelling</strong> of this geometry in terms of the objects and regions that are known to exist in the scene</li>
</ul>
<p>However, this maybe reveals <strong>a huge amount of wasted computational effort</strong></p>
<ul>
<li>The potential for a much better way of <strong>taking account of domain knowledge in the loop of SLAM operation itself</strong></li>
<li>Object SLAM has many characteristics of a return to feature-based SLAM methods</li>
</ul>
<hr>
<h2 id="introduction---proposed">Introduction - Proposed</h2>
<p>This paper propose</p>
<ul>
<li>a paradigm for <strong>real-time localisation</strong> and</li>
<li><strong>mapping</strong> which harnesses <strong>3D object recognition</strong> to <strong>jump over low level geometry processing</strong> and</li>
<li><strong>produce</strong> incrementally built <strong>maps</strong> directly at the <strong>object oriented</strong> level.</li>
</ul>
<hr>
<h2 id="slam---a-pure-object-level-slam-system">SLAM++ - A Pure Object-Level SLAM System</h2>
<p><img align="right" width="60%" height="60%" src="Assets/images/papers/SLAM++/SLAMPlusPlus_A_Pure_Object-Level_SLAM_System.png"></p>
<p>L: A live view at the <strong>current camera pose</strong> and the <strong>synthetic rendered objects</strong> &lt;/br&gt;</p>
<p>R: Contrast <strong>a raw depth camera normal map</strong> with <strong>the corresponding high quality prediction</strong> from object graph</p>
<hr>
<h2 id="mapping-directly-at-the-object-level">Mapping Directly at the Object Level</h2>
<h2 id="advantages">Advantages:</h2>
<ol>
<li>Instant <strong>semantic understanding</strong> enables <strong>interaction</strong> (Robotics, AR)</li>
<li>Dense representation and full predictive power of <strong>KinectFusion</strong> but <strong>very memory-efficient</strong></li>
<li>Some aspects of a return to feature-based paradigm &#x2014; full joint <strong>optimisation of a graph</strong> in real-time is feasible</li>
</ol>
<hr>
<h2 id="system-overview">System Overview</h2>
<center>
<img height="90%" width="90%" src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png">
</center>

<p><strong>Real-Time Loop</strong>: &lt;/br&gt;</p>
<ul>
<li>Currently uses depth data only (no RGB).</li>
<li>Most components implemented primarily on GPU (not the pose graph optimisation which uses g2o).</li>
</ul>
<hr>
<h2 id="methods---overview">Methods - Overview</h2>
<p>$D_l$: a live depth map;  $N_l$ :  normal map
First compute  a surface measurement in the form of a vertex and $N_l$ providing to the sequentially computed camera tracking and object detection pipelines</p>
<p><img src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png" alt=""></p>
<hr>
<h2 id="method---overview---step--1">Method - Overview - Step  1</h2>
<p><strong>Track the live camera pose $T_{wl}$</strong> with an iterative closest point (ICP) approach using the dense multi-object scene prediction  captured in the current SLAM graph $G$
<img src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png" alt=""></p>
<hr>
<h2 id="method---overview---step-2">Method - Overview - Step 2</h2>
<p>Attempt to <strong>detect objects</strong>, previously stored in a database, that are present in the live frame, <strong>generating detection candidates</strong> with <strong>an estimated pose</strong> in the scene. Candidates are first refined or rejected using a second ICP estimation initialised with the detected pose
<img src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png" alt=""></p>
<hr>
<h2 id="method---overview---step-3">Method - Overview - Step 3</h2>
<p>Add successfully <strong>detected objects $g$</strong> into the SLAM graph in the form of a <strong>object-pose</strong> vertex connected to the live estimated <strong>camera-pose</strong> vertex via a measurement edge
<img src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png" alt=""></p>
<hr>
<h2 id="method---overview---step-4">Method - Overview - Step 4</h2>
<p>Rendering objects from the SLAM graph produce a <strong>predicted depth $D_r$</strong> and <strong>normal map $N_r$</strong> into the live estimated frame, enabling us to actively <strong>search only those pixels not described by current objects in the graph</strong>. Run an individual ICP between each object and the live image resulting in the addition of a new camera-object constraint into the SLAM graph.
<img src="Assets/images/papers/SLAM++/Outline_of_the_SLAM++_pipeline.png" alt="55% center"></p>
<hr>
<h2 id="object-database">Object Database</h2>
<p><img align="right" height="40%" width="40%" src="Assets/images/papers/SLAM++/SLAM++-Object_Database.png"></p>
<p><strong>Currently a small number of precisely known objects</strong>:</p>
<p>Carefully scanned using <strong>KinectFusion</strong> and manually segmented</p>
<hr>
<h2 id="real-time-object-detection">Real-Time Object Detection</h2>
<figure id="fig3.4.1"><img src="Assets/images/papers/SLAM++/SLAM++-RealTimeObjectDetection.png" alt="center 60%"><figcaption>Image 3.4.1 - center 60%</figcaption></figure>
<p>From &#x2018;Model Globally, Match Locally&#x2019;, Drost et al., CVPR 2010</p>
<ul>
<li>Each object is described in terms of all point-pair features (PPF) across their whole 3D surface</li>
<li>Point pair features are grouped and stored in a hash table</li>
</ul>
<hr>
<h2 id="real-time-object-detection---ppf">Real-Time Object Detection - PPF</h2>
<center>
<img align="middle" height="80%" width="80%" src="Assets/images/papers/SLAM++/SLAM++-RealTimeObjectDetection.png">
 </center>

<p>$F$: PPF of two oriented points
$F_1$: the distance of the points
$F_2$: the angle between the normals and $F_1$
$F_4$: the angle between the two normals</p>
<hr>
<h2 id="real-time-object-detection">Real-Time Object Detection</h2>
<center>
<img align="middle" height="80%" width="80%" src="Assets/images/papers/SLAM++/SLAM++-RealTimeObjectDetection.png">
 </center>

<p>The global model description
Left: Point pairs on the model surface with <strong>similar feature vector $F$</strong>
Right: Those point pairs are stored in the same slot in the hash table</p>
<hr>
<h2 id="icp-algorithm-overview">ICP Algorithm Overview</h2>
<figure id="fig3.4.2"><img src="Assets/images/papers/SLAM++/ICP_breif_description.png" alt="50% center"><figcaption>Image 3.4.2 - 50% center</figcaption></figure>
<ul>
<li><strong>Idea</strong>: iterate to find alignmen</li>
<li><strong>Wanted</strong>: translation t and rotation R that minimizes the sum of the squared error:</li>
</ul>
<p><img src="Assets/images/papers/SLAM++/ICP_Minimum_Error.png" alt="50% center">
Where  $x_i$ and $p_i$ are corresponding points</p>
<hr>
<h2 id="icp-refinement">ICP Refinement</h2>
<p><img align="right" height="60%" width="60%" src="Assets/images/papers/SLAM++/SLAM++-ICPReinformance.png"></p>
<p>Noisy raw detected poses refined with ICP against object model:</p>
<ul>
<li>False detections removed with threshold after ICP</li>
<li>Accurate object-camera constraints achieved and added to graph</li>
</ul>
<hr>
<h2 id="camera-tracking">Camera Tracking</h2>
<p>Camera is tracked using ICP against a depth map raycasted from the current whole world model at last pose:</p>
<ul>
<li>Very much the same as KinectFusion; but we track against our perfect, <strong>complete object models</strong></li>
<li>Full prediction and <strong>occlusion handling</strong>, even for recently acquired objects.</li>
</ul>
<hr>
<h2 id="graph-optimisation">Graph Optimisation</h2>
<p><img align="right" src="Assets/images/papers/SLAM++/Example_graph_illustrating.png"></p>
<ul>
<li>Reprsent the world as a graph</li>
<li>Each node stores estimated $SE(3)$ pose (rotation and translation relative to a fixed world frame)</li>
</ul>
<p><strong>Goal</strong>: minimize the sum over all measurement constraints</p>
<hr>
<h2 id="graph-optimisation-presentation">Graph Optimisation Presentation</h2>
<p><img align="right" src="Assets/images/papers/SLAM++/Example_graph_illustrating.png"></p>
<ul>
<li>$T_{woj}$:  $SE(3)$ pose of object $j$</li>
<li>$T_{wi}$:  SE(3) pose of the historical pose of the camera at timestep $i$.</li>
<li>$Z_{i,oj}$: constraints, which links one camera pose and one object pose</li>
</ul>
<hr>
<h2 id="map-representation">Map Representation</h2>
<p><img align="right" height="60%" width="60%" src="Assets/images/papers/SLAM++/SLAM++-Map_reprsentation.png"></p>
<p><strong>A Pure Graph of Objects</strong>:</p>
<p>A pose graph where all constraints are 6DoF</p>
<hr>
<h2 id="moved-object-detection">Moved Object Detection</h2>
<p><img align="right" height="60%" width="60%" src="Assets/images/papers/SLAM++/SLAM++-MovedObjectDetection.png"></p>
<p>This paper demonstrate the ability to detect the movement of objects, which fail ICP gating due to inconsistency</p>
<hr>
<h2 id="relocalisation">Relocalisation</h2>
<p><img align="right" height="50%" width="50%" src="Assets/images/papers/SLAM++/Relocalization_procedure.png">
 When camera tracking is lost the system enters a relocalisation mode.</p>
<p> The matched vertex with highest vote in the long-term graph is used instead of the currently observed vertex in the local graph and camera tracking is resumed from it, discarding the local map.</p>
<hr>
<h2 id="augmented-reality-with-objects">Augmented Reality with Objects</h2>
<figure id="fig3.4.3"><img src="Assets/images/papers/SLAM++/AugmentedRealityWithObjects.png" alt="center 80%"><figcaption>Image 3.4.3 - center 80%</figcaption></figure>
<p>The ability to semantically predict complete surface geometry from partial views allows novel context aware AR capabilities such as path finding, to gracefully avoid obstacles while reaching target objects</p>
<hr>
<h1 id="any-questions">Any Questions?</h1>
<hr>
<h1 id="conclusions">Conclusions</h1>
<ul>
<li>Using <strong>high performance 3D object recognition</strong> in the loop permits a new approach to real-time SLAM with large advantages in terms of <strong>efficient and semantic scene description</strong>.</li>
<li>Suit to locations like <strong>the interiors of public buildings</strong> with many <strong>repeated, identical elements</strong>.</li>
</ul>
 <link rel="stylesheet" type="text/css" href="https://storage.googleapis.com/app.klipse.tech/css/codemirror.css"> <script>     window.klipse_settings = {         selector: ".language-klipse, .lang-eval-clojure",         selector_eval_js: ".lang-eval-js",         selector_eval_python_client: ".lang-eval-python",         selector_eval_php: ".lang-eval-php",         selector_eval_scheme: ".lang-eval-scheme",         selector_eval_ruby: ".lang-eval-ruby",         selector_reagent: ".lang-reagent",        selector_google_charts: ".lang-google-chart",        selector_es2017: ".lang-eval-es2017",        selector_jsx: ".lang-eval-jsx",        selector_transpile_jsx: ".lang-transpile-jsx",        selector_render_jsx: ".lang-render-jsx",        selector_react: ".lang-react",        selector_eval_markdown: ".lang-render-markdown",        selector_eval_lambdaway: ".lang-render-lambdaway",        selector_eval_cpp: ".lang-eval-cpp",        selector_eval_html: ".lang-render-html",        selector_sql: ".lang-eval-sql",        selector_brainfuck: "lang-eval-brainfuck",        selector_js: ".lang-transpile-cljs"    }; </script> <script src="https://storage.googleapis.com/app.klipse.tech/plugin/js/klipse_plugin.js"></script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Fusion++.html" class="navigation navigation-prev " aria-label="Previous page: Fusion++">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html" class="navigation navigation-next " aria-label="Next page: Monocular Object and Plane SLAM in Structured Environments">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"SLAM++","level":"3.4","depth":1,"next":{"title":"Monocular Object and Plane SLAM in Structured Environments","level":"3.5","depth":1,"path":"Monocular-Object-and-Plane-SLAM-in-Structured-Environments.md","ref":"Monocular-Object-and-Plane-SLAM-in-Structured-Environments.md","articles":[]},"previous":{"title":"Fusion++","level":"3.3","depth":1,"path":"Fusion++.md","ref":"Fusion++.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{"_pictures":[{"backlink":"SLAM-Resources.html#fig1.3.1","level":"1.3","list_caption":"Figure: Georg Klein","alt":"Georg Klein","nro":1,"url":"http://www.robots.ox.ac.uk/~gk/imgs/georg_klein_136.jpg","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Georg Klein","attributes":{},"skip":false,"key":"1.3.1"},{"backlink":"SLAM++.html#fig3.4.1","level":"3.4","list_caption":"Figure: center 60%","alt":"center 60%","nro":2,"url":"/Assets/images/papers/SLAM++/SLAM++-RealTimeObjectDetection.png","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"center 60%","attributes":{},"skip":false,"key":"3.4.1"},{"backlink":"SLAM++.html#fig3.4.2","level":"3.4","list_caption":"Figure: 50% center","alt":"50% center","nro":3,"url":"/Assets/images/papers/SLAM++/ICP_breif_description.png","index":2,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"50% center","attributes":{},"skip":false,"key":"3.4.2"},{"backlink":"SLAM++.html#fig3.4.3","level":"3.4","list_caption":"Figure: center 80%","alt":"center 80%","nro":4,"url":"/Assets/images/papers/SLAM++/AugmentedRealityWithObjects.png","index":3,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"center 80%","attributes":{},"skip":false,"key":"3.4.3"},{"backlink":"kinect2.html#fig7.2.1","level":"7.2","list_caption":"Figure: Kinetic2 Demo","alt":"Kinetic2 Demo","nro":5,"url":"../Assets/Images/Kinetic2_Protonect_Test_Result.jpg","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Kinetic2 Demo","attributes":{},"skip":false,"key":"7.2.1"},{"backlink":"slam-course-robot-mapping.html#fig8.3.1","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":6,"url":"/assets/Estimate the Robot Pose.png","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.1"},{"backlink":"slam-course-robot-mapping.html#fig8.3.2","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":7,"url":"/assets/Estimate Landmarks.png","index":2,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.2"},{"backlink":"slam-course-robot-mapping.html#fig8.3.3","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":8,"url":"/assets/Screen Shot 2019-01-28 at 7.20.23.png","index":3,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.3"},{"backlink":"slam-course-robot-mapping.html#fig8.3.4","level":"8.3","list_caption":"Figure: Uncertainty Representation","alt":"Uncertainty Representation","nro":9,"url":"/assets/Screen Shot 2019-01-28 at 8.14.04.png","index":4,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Uncertainty Representation","attributes":{},"skip":false,"key":"8.3.4"},{"backlink":"slam-course-robot-mapping.html#fig8.3.5","level":"8.3","list_caption":"Figure: SLAM in the Probabilistic World","alt":"SLAM in the Probabilistic World","nro":10,"url":"/assets/Screen Shot 2019-01-28 at 8.39.27.png","index":5,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"SLAM in the Probabilistic World","attributes":{},"skip":false,"key":"8.3.5"},{"backlink":"slam-course-robot-mapping.html#fig8.3.6","level":"8.3","list_caption":"Figure: Motion and Observation Model","alt":"Motion and Observation Model","nro":11,"url":"assets/markdown-img-paste-20190128111722554.png","index":6,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion and Observation Model","attributes":{},"skip":false,"key":"8.3.6"},{"backlink":"slam-course-robot-mapping.html#fig8.3.7","level":"8.3","list_caption":"Figure: Motion Model","alt":"Motion Model","nro":12,"url":"assets/markdown-img-paste-20190128112352359.png","index":7,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion Model","attributes":{},"skip":false,"key":"8.3.7"},{"backlink":"slam-course-robot-mapping.html#fig8.3.8","level":"8.3","list_caption":"Figure: Motion Model Example","alt":"Motion Model Example","nro":13,"url":"assets/markdown-img-paste-20190128111616903.png","index":8,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion Model Example","attributes":{},"skip":false,"key":"8.3.8"},{"backlink":"slam-course-robot-mapping.html#fig8.3.9","level":"8.3","list_caption":"Figure: Standard Odometry Model","alt":"Standard Odometry Model","nro":14,"url":"assets/markdown-img-paste-20190128112933104.png","index":9,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Standard Odometry Model","attributes":{},"skip":false,"key":"8.3.9"},{"backlink":"slam-course-robot-mapping.html#fig8.3.10","level":"8.3","list_caption":"Figure: Observation Model","alt":"Observation Model","nro":15,"url":"assets/markdown-img-paste-20190128114114463.png","index":10,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Observation Model","attributes":{},"skip":false,"key":"8.3.10"},{"backlink":"slam-course-robot-mapping.html#fig8.3.11","level":"8.3","list_caption":"Figure: Observation Model Examples","alt":"Observation Model Examples","nro":16,"url":"assets/markdown-img-paste-20190128114225690.png","index":11,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Observation Model Examples","attributes":{},"skip":false,"key":"8.3.11"}]},"plugins":["multipart","youtube","image-captions","disqus","donate","advanced-emoji","splitter","mermaid-gb3","puml","graph","chart","simple-page-toc","sitemap-general","todo","terminal","copy-code-button","include-csv","klipse","scripts","page-numbering","codeblock-filename","katex","livereload"],"pluginsConfig":{"include-csv":{},"disqus":{"useIdentifier":false,"shortName":"https-yubaoliu-gitbooks-io"},"youtube":{},"puml":{},"livereload":{},"simple-page-toc":{},"todo":{},"splitter":{},"scripts":{"files":["./myscript.js"]},"search":{},"multipart":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"graph":{},"donate":{"alipay":"","alipayText":"支付宝捐赠","button":"Donate","title":"","wechat":"https://i.imgur.com/nUAbMLG.png","wechatText":"微信捐赠"},"sitemap-general":{"prefix":"https://yubaoliu.gitbooks.io"},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"codeblock-filename":{},"copy-code-button":{},"klipse":{"myConfigKey":"it's the default value"},"page-numbering":{"chapterFormat":"Chapter #chapno#&nbsp;&nbsp;&nbsp;#title# ||| #chapno#&nbsp;&nbsp;&nbsp;#title#","forceMultipleParts":false,"skipReadme":false},"advanced-emoji":{"embedEmojis":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"terminal":{"copyButtons":true,"fade":false,"style":"flat"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"chart":{"type":"c3"},"image-captions":{"caption":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","variable_name":"_pictures"}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"SLAM++.md","mtime":"2019-01-25T02:46:52.868Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-01-30T02:01:12.728Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/URI.js/1.16.1/URI.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-disqus/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-donate/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-terminal/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-scripts/9f6c471c34065cdbabd862bf00229682-myscript.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    <script src="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>

    </body>
</html>

